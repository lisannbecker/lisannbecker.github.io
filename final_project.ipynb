{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a9a5d7",
   "metadata": {},
   "source": [
    "# Final project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c5e30",
   "metadata": {},
   "source": [
    "## Project title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437cd6b6",
   "metadata": {},
   "source": [
    "Research Question: Which UvA courses do I meet the entry requirements for? XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c7452",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Process Book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c909a7a",
   "metadata": {},
   "source": [
    "1. Don’t forget the process book. Maintain it every day you’ve worked on the project. This is the main way for \n",
    "   us to see what you’ve learned and is an important part of the evaluation.\n",
    "2. Don’t use it for everything. Don’t spend too much time on your process book! Ten minutes \n",
    "   every day you’ve worked on the project on this, should be more than enough.\n",
    "3. Keep it simple and short. Use simple language, short sentences, and be as straightforward as possible when \n",
    "   documenting your learning process.\n",
    "4. Create a simple template. In order to organize your process book, it is nice to create and use a template:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab0e988",
   "metadata": {},
   "source": [
    "### Date of entry: 25. Jan\n",
    "#### What I’ve worked on: \n",
    "Git\n",
    "- Lecture git\n",
    "- Tried out git by myself\n",
    "- Created git repository\n",
    "\n",
    "Project choice\n",
    "- Learn to use tableau and uber data as I am applying to intern at uber and tableau is a desired skill\n",
    "\n",
    "Tableau\n",
    "- Downloaded tableau \n",
    "- See what I can use tableau for to decide which project I want to do specifically\n",
    "- Learn tableau basics\n",
    "- Experiment with DP datasets (e.g., weather data) in tableau\n",
    "- Experiment with uber selection dataset --> too easy (?)\n",
    "    \n",
    "#### What problems I encountered:\n",
    "- Cannot use dataset I first thought about using\n",
    "\n",
    "#### What I learned:\n",
    "- How to use git\n",
    "- Tableau basics https://www.youtube.com/watch?v=jEgVto5QME8\n",
    "- I want to gather my own uber data e.g., through web-scraping, or API\n",
    "\n",
    "#### Which resources did I use:\n",
    "- Course material, YouTube, tableau, tableau user guide, uber data XXX\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Date of entry: 26. Jan\n",
    "#### What I’ve worked on: \n",
    "Tableau\n",
    "- Learned more functions of tableau and experimented with these  \n",
    "\n",
    "Project\n",
    "- Created jupyter notebook\n",
    "- Started process book\n",
    "\n",
    "Brainstorm\n",
    "- Potentially make website with tableau dashboard\n",
    "- Which data can I use? \n",
    "\n",
    "Find data\n",
    "- Look for uber APIs\n",
    "\n",
    "#### What problems I encountered: \n",
    "- Uber APIs are not open, I can not use them \n",
    "\n",
    "#### What I learned: \n",
    "Tableau\n",
    "- Basic functions: https://www.youtube.com/watch?v=yejulE4b-3Y\n",
    "- Use and join (csv) files: https://www.youtube.com/watch?v=5WXrJ1JvTOo\n",
    "- Integrate tableau dashboard in website: https://www.youtube.com/watch?v=wJ2CHIJalNU\n",
    "\n",
    "#### Which resources did I use:\n",
    "- Course material, YouTube, tableau, tableau user guide\n",
    "\n",
    "\n",
    "\n",
    "### Date of entry: 27. Jan\n",
    "#### What I’ve worked on: \n",
    "- Find new project ideas\n",
    "- Decided for final topic/Question\n",
    "- Changed strategy for scraping data: work with individual course pages instead of course overview\n",
    "\n",
    "#### What problems I encountered: \n",
    "- Url of studiegids does not change per page of courses\n",
    "\n",
    "#### What I learned: \n",
    "\n",
    "#### Which resources did I use:\n",
    "- Multiple forms to try to solve the url problem, but didn't find a solution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Date of entry: 30. Jan\n",
    "#### What I’ve worked on: \n",
    "- Scraping course data: name of course, url, fiel of studies, education level, entry requirements\n",
    "- Started transforming the entry requirements text into categorical entry requirement checks that students can filter for\n",
    "\n",
    "#### What problems I encountered: \n",
    "- I cannot simply check whether a unique number-combination in the end of the link opens a specific course page by testing if the link is dead or not: if the number in the end doesn't link to a course it still opens the studigids page\n",
    "- The entry requirement texts are very heterogenous and need a lot of transformation. This will become the most time-consuming part of scraping\n",
    "\n",
    "\n",
    "#### What I learned: \n",
    "- New commands for text transformation\n",
    "\n",
    "#### Which resources did I use:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Date of entry: 31. Jan\n",
    "#### What I’ve worked on: \n",
    "Refined scraping entry requirements (scraper 2):\n",
    "- Added function (1) to prepare/transform entry requirements text for further functions\n",
    "- Added function (2) to extract courses and with specific credit requirements\n",
    "- Added function (3) to extract all courses required \n",
    "- Combined these functions in one entry requirements-scraper\n",
    "\n",
    "#### What problems I encountered: \n",
    "- Function (3) often required adjustments as it recognised some 'course names' incorrectly\n",
    "- I could integrate a feedback function\n",
    "\n",
    "#### What I learned: \n",
    "\n",
    "#### Which resources did I use:\n",
    "- nltk to detect stopwords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Date of entry: 1. Feb\n",
    "#### What I’ve worked on: \n",
    "Refined scraping entry requirements (scraper 2):\n",
    "- Improved function (2) to extract courses and with specific credit requirements\n",
    "- Experimented with combining scraper part 1 with scraper part 2 (entry requirements)\n",
    "\n",
    "#### What problems I encountered: \n",
    "- Transform entry requirements text to categories: Special cases for scraping course that has credit requirement after the course name ('course name (XX EC)') instead of before ('XX ec in course name)\n",
    "\n",
    "#### What I learned: \n",
    "- New commands for text processing\n",
    "- New ways of slicing/working with lists\n",
    "\n",
    "#### Which resources did I use:\n",
    "\n",
    "\n",
    "\n",
    "### Date of entry: 2. Feb\n",
    "#### What I’ve worked on: \n",
    "- Making final combined scraper\n",
    "- Fixing entry requirements errors\n",
    "\n",
    "#### What problems I encountered: \n",
    "\n",
    "#### What I learned: \n",
    "\n",
    "#### Which resources did I use:\n",
    "\n",
    "\n",
    "forums\n",
    "stackoverflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccfb7cb",
   "metadata": {},
   "source": [
    "## To-Do\n",
    "\n",
    "1. Scrape / Transform data \n",
    "2. Import data into tableau\n",
    "3. (Webpage with tableau dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3792a05",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23eecd3",
   "metadata": {},
   "source": [
    "Relevant personal data\n",
    "\n",
    "Programme: \"\"\n",
    "Level: Bachelor's, Master's (Master's, Master), Exchange programme ('Exchange Programme Exchange Programme UvA', 'Exchange Programme Exchange Programme'), pre-Master's programme, Secondary Subject/Bijvak('?'), 'Honours MSc', 'Research Master's\n",
    "Year of the studies: \"\"\n",
    "Credits obtained this/1st year: \"\"\n",
    "\n",
    "url: https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course\n",
    "\n",
    "\n",
    "Relevant course information \n",
    "\n",
    "Course name: Course name\n",
    "Entry requirements: Course > Entry requirements > Split by sentence & look for keywords (if not present, add to dict with unknown entry requirements)\n",
    "Level: Course > Is part of > loop through Level list (s.a.) \n",
    "Field: Course > Is part of > remove Level > everything left \n",
    "\n",
    "(Coordinator name: Course > Lecturer(s) > Coordinator ('co-ordinator')\n",
    "(Coordinator email: Course > Lecturer(s) > Coordinator (crawl) > Email)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "60938e7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'validators'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [95], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestException\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m closing\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvalidators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ValidationFailure\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'validators'"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b99267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare webscraping with BeautifulSoup\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'\n",
    "        }\n",
    "        with closing(get(url, stream=True, headers=headers)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "    except RequestException as e:\n",
    "        print('The following error occurred during HTTP GET request to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns true if the response seems to be HTML, false otherwise\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200\n",
    "            and content_type is not None\n",
    "            and content_type.find('html') > -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b3b86",
   "metadata": {},
   "source": [
    "- All courses have individual descriptions with unique urls\n",
    "- First, I will find all urls that link to courses by trying out urls\n",
    "- I can check whether I found all courses by comparing the length of my url list to the number or courses stated on the website\n",
    "- After that, I will loop through the urls that work and scrape the data for all courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8d2cff15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Work, Power & Income:\\xa0Why regulation fails to address labour market inequality (&sometimes succeeds)': {'url': 'https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/102591', 'Academic level': [\"Bachelor's\"], 'Field of study': ['Politics, Psychology, Law and Economics (PPLE)'], 'Entry requirements': []}, 'American Think Tanks and U.S. Foreign Policy ': {'url': 'https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/102593', 'Academic level': ['Exchange programme Exchange Programme'], 'Field of study': ['Graduate School of Humanities (MA)'], 'Entry requirements': []}, '\"Losing Earth\"?: Activism and Diplomacy on the Environment and Climate since 1968': {'url': 'https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/102595', 'Academic level': ['Exchange programme Exchange Programme'], 'Field of study': ['Graduate School of Humanities (MA)'], 'Entry requirements': []}}\n"
     ]
    }
   ],
   "source": [
    "#SCRAPER PART 1: name, urls, levels and fields (subset of the data) \n",
    "\n",
    "potential_levels = [\"Bachelor's\", \"Dual Master's\", \"Master's\", \"Exchange Programme Exchange Programme UvA\", \n",
    "                    \"Exchange programme Exchange Programme\", \"Exchange programme Exchange Programme UvA\", \n",
    "                    \"Exchange Programme Exchange Programme\", \"pre-Master's programme\", \"Secondary Subject\", \n",
    "                    \"Bijvak\", \"Honours MSc\", \"Research Master's\", \"Minor\"] #tailored to english website\n",
    "\n",
    "courses = {}\n",
    "unique_n = 102590\n",
    "    \n",
    "while unique_n < 102600:\n",
    "    url = f'https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/{unique_n}'\n",
    "    html = simple_get(url)\n",
    "    dom = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    unique_n += 1\n",
    "    \n",
    "    if (dom.find('article', class_ = 'twelve columns')) is None: #no course under this url - go to next url\n",
    "        pass\n",
    "    \n",
    "    else: #course exists\n",
    "        name = dom.find('article', class_ = 'twelve columns').h1.text[1::]\n",
    "        courses[name] = {}\n",
    "        #print(name)\n",
    "        courses[name]['url'] = url\n",
    "        courses[name]['Academic level'] = [] #Add empty list in case there is no data\n",
    "        courses[name]['Field of study'] = [] #Add empty list in case there is no data\n",
    "        courses[name]['Entry requirements'] = [] #Add empty list in case there is no data\n",
    "\n",
    "        #print(url)\n",
    "        \n",
    "        for info_block in dom.find_all('tr', {'class':''}):\n",
    "            info_block = info_block.text\n",
    "\n",
    "\n",
    "            #Education level and field of the course\n",
    "            if 'Is part of' in info_block: #Find text box that contains level and field of the course\n",
    "                is_part_of = info_block.replace(\"Is part of\", \"\") #Remove title of the text box\n",
    "\n",
    "                # Education level of the course e.g., Bachelor's\n",
    "                levels = [] \n",
    "                for l in potential_levels:\n",
    "                    if l in is_part_of:\n",
    "                        levels.append(l)\n",
    "\n",
    "                # Field(s) of the course e.g., Business\n",
    "                for lev in levels: \n",
    "                    is_part_of = is_part_of.replace(lev+\" \", \";\") #replace education level by semicolon to split on the semicolon later\n",
    "                fields = is_part_of[1::].split(\";\") #remove first semicolon to not get an empty element in the study fields list \n",
    "                                                    #and split on semicolon to get this list\n",
    "        \n",
    "                courses[name]['Academic level'] = levels\n",
    "                courses[name]['Field of study'] = fields\n",
    "\n",
    "        \n",
    "        #entry = \n",
    "\n",
    "print(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indicators = 'priority', 'research participation', 'only', 'selected into', \n",
    "#              'completed', 'major/minor', 'major', 'minor', 'elective' XXX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc662aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run before testing single functions\n",
    "\n",
    "url = f'https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/101959'\n",
    "html = simple_get(url)\n",
    "dom = BeautifulSoup(html, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "52861f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['60 ec from the first-year Psychology curriculum, including Research Participation', 'Scientific and Statistical Reasoning (7202A701XY), Practical training: Psychological Research (7202A704), as well as ≥ 18 ec of the course package of either Brain & Cognition or Psychological Methods', 'For Brain & Cognition students, the 18 EC must include the course B&C Toolbox: Experimental and Neuroscientific Methods (7202BP04)']\n"
     ]
    }
   ],
   "source": [
    "#Test: clean_entry function\n",
    "\n",
    "upper = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "for info_block in dom.find_all('tr', {'class':''}):\n",
    "    info_block = info_block.text\n",
    "    \n",
    "    # Entry requirements of the course: full text\n",
    "    if 'Entry requirements' in info_block: #Find text box with entry requirements\n",
    "        entry = info_block.replace(\"Entry requirements\", \"\") #Remove title of the text box\n",
    "        entry = ' '.join(entry.split()) #remove multiple whitespaces by splitting and joining\n",
    "\n",
    "        #Split into sentences as these contain seperate criteria\n",
    "        entry = entry.replace(\". \", \".\").split(\".\")\n",
    "        entry = list(filter(None, entry))\n",
    "\n",
    "print(entry)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "68151700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['60 ec from the first-year Psychology curriculum, including Research Participation', 'Scientific and Statistical Reasoning (7202A701XY), Practical Training: Psychological Research (7202A704), as well as ≥ 18 ec of the course package of either Brain & Cognition or Psychological Methods', 'For Brain & Cognition students, the 18 ec must include the course B&C Toolbox: Experimental and Neuroscientific Methods (7202BP04)']\n"
     ]
    }
   ],
   "source": [
    "# LOWERCASE FIRST/SECOND/THIRD YEAR AND ECS - finished\n",
    "\n",
    "def clean_entry(entry):\n",
    "    low = ['Year', 'EC', 'First', 'Second', 'Third', 'First-Year', 'First-year', 'first-Year', # List of words that need to be lowercase\n",
    "           'Second-Year', 'Second-year', 'second-Year', 'Third-Year', 'Third-year', 'third-Year',\n",
    "           'EC', 'ECTs', 'Credits', 'ECTS', 'ECT', 'Ect', 'Ec', 'ECs', 'Ecs'] \n",
    "    low_2 = ['EC)', 'ECTs)', 'Credits)', 'ECTS)', 'ECT)', 'Ect)', 'Ec)', 'ECs)', 'Ecs)'] \n",
    "    low_3 = ['EC),', 'ECTs),', 'Credits),', 'ECTS),', 'ECT),', 'Ect),', 'Ec),', 'ECs),', 'Ecs),']\n",
    "    error_wordsl = ['training:'] #words I found that were incorrectly lowercased\n",
    "\n",
    "    entry_c = '' #transformed entry\n",
    "    #print(entry)\n",
    "\n",
    "    \n",
    "    for sentence in entry:\n",
    "        words = sentence.split() #split entry into sentences and into words\n",
    "\n",
    "        for word in words:\n",
    "            if word in low:\n",
    "                word_l = word.lower() #Transform first/second/third year to lowercase so it doesn't interrupt the course title detection\n",
    "                entry_c += word_l+\" \"\n",
    "            elif word in low_2:\n",
    "                word_l = word.lower()\n",
    "                word_l = word_l[:-1]\n",
    "                entry_c += word_l+\" \"\n",
    "            elif word in low_3:\n",
    "                word_l = word.lower()\n",
    "                word_l = word_l[:-2] #XXX offene Klammer; dann nach links anstatt rechts\n",
    "                entry_c += word_l+\" \"\n",
    "            elif word in error_wordsl:\n",
    "                word_l = word[0].upper() + word[1:]\n",
    "                entry_c += word_l+\" \"\n",
    "            else:\n",
    "                entry_c += word+ \" \"\n",
    "        entry_c = entry_c[:-1]+\".\" #remove last space\n",
    "\n",
    "    entry_c = entry_c.split('.') #turn entry form full-text into list of sentences again\n",
    "    entry_c = list(filter(None, entry_c)) #remove empty elements\n",
    "\n",
    "    return entry_c\n",
    "\n",
    "entry = clean_entry(entry)\n",
    "print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b419a275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'academic', 'year', 'in', 'Anthropology', '(60', 'ec', 'Theory', 'and', 'History', 'of', 'Anthropology', '(THA)', 'and/or', 'Historical', 'and', 'Comparative', 'Sociology', '(HCS),', 'a', 'Specialisation', 'Course', 'and', 'Practicing', 'Ethnography']\n"
     ]
    }
   ],
   "source": [
    "#Test: find_c_names function\n",
    "\n",
    "upper = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "#sentence = 'Scientific and Statistical Reasoning (7202A701XY), Practical Training: Psychological Research (7202A704), ≥ 18 ec of the course package Clinical Psychology and the first part of Evidence-based Clinical Practice: Research Methods/CP (72037001).'\n",
    "sentence = '60 ec from the first-year Psychology curriculum, including Research Participation'\n",
    "sentence = 'Scientific and Statistical Reasoning (7202A701XY), Practical Training: Psychological Research (7202A704), ≥ 18 ec of the course package Clinical Psychology and the first part of Evidence-based Clinical Practice: Research Methods/CP (72037001)'\n",
    "sentence = 'first academic year in Anthropology (60 ec Theory and History of Anthropology (THA) and/or Historical and Comparative Sociology (HCS), a Specialisation Course and Practicing Ethnography'\n",
    "words = sentence.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eb097dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For Brain & Cognition', 'B&C Toolbox: Experimental and Neuroscientific Methods']\n"
     ]
    }
   ],
   "source": [
    "#FUNCTION: COURSE TITLES - finished\n",
    "\n",
    "\n",
    "\n",
    "# Extract list of course titles from the entry requirements text; based on capitalisation; sensitive to words\n",
    "# that are typically not capitalised in titles (conjunctions, articles, prepositions)\n",
    "\n",
    "\n",
    "def find_c_names(words): #input: list of words of one sentence from the entry requirements text\n",
    "    \n",
    "    # List of words not capitalised in headings\n",
    "    conjunctions = [\"and\", \"or\", \"but\", \"nor\", \"yet\", \"so\", \"for\", \"&\"]\n",
    "    articles = ['a', 'an', 'the']\n",
    "    prepositions = [\"in\", \"to\", \"of\", \"at\", \"by\", \"up\", \"for\", \"off\", \"on\"]\n",
    "    years = ['year', 'first-year', 'second-year', 'third-year'] \n",
    "    years_cond = ['first', 'second', 'third', '1st', '2nd', '3rd', 'academic', 'Academic'] #needs to be separate because of problem: Clinical Psychology and the first\n",
    "    no_cap = conjunctions + articles + prepositions\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\")) #set of english stopwords\n",
    "    error_wordsu = ['Students', 'Course', '“Recommended', 'Knowledge”'] #words that were incorrectly capitalised\n",
    "\n",
    "    \n",
    "    \n",
    "    string = ''\n",
    "    \n",
    "    # Find wordgroups of nouns and conjunctions/articles/prepositions\n",
    "    for i in range(len(words)):\n",
    "        if words[i][0] in upper or words[i] in no_cap or words[i] in years or words[i] in years_cond:\n",
    "            string += words[i] + \" \"\n",
    "            #print(string)\n",
    "        else:\n",
    "            string = string[:-1]\n",
    "            string += \"=\" \n",
    "    string = string[:-1] #remove last split sign (=)\n",
    "    c_names_raw = string.split('=')\n",
    "    #print(f'c_names_raw: {c_names_raw}')\n",
    "    \n",
    "    c_names = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Delete non-capitalised words that embed capitalised words - these are likely not part of the course title (e.g., 'of the', 'Clinical Psychology and the', 'of Evidence-based Clinical Practice: Research Methods/CP')\n",
    "    for ele in c_names_raw:\n",
    "        \n",
    "        words = ele.split()\n",
    "        #print(f'words:{words}')\n",
    "        \n",
    "        while len(words) != 0 and words[0][0] not in upper and words[0] not in years and words[0] not in years_cond: #delete no cap words in the beginning\n",
    "            del words[0]\n",
    "\n",
    "        while len(words) != 0 and words[-1][0] not in upper and words[-1] not in years: #delete no cap words in the end and 'first'/'second'/'third' if not used in context of a year\n",
    "            del words[-1]\n",
    "\n",
    "        if len(words) == 1 and words[0].lower() in no_cap or len(words) == 1 and words[0].lower() in stop_words or len(words) == 1 and words[0] in error_wordsu: #delete single no cap words and stop words that were capitalised as they opened a sentence XXXtechnically, all verbs and adjectives are probably not course titles\n",
    "            del words[0]\n",
    "            \n",
    "        ele = ' '.join(words)\n",
    "        \n",
    "        if len(ele) != 0: #do not append empty course names\n",
    "            c_names.append(ele)\n",
    "\n",
    "            \n",
    "    return c_names\n",
    "\n",
    "courses = find_c_names(words)\n",
    "print(courses)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcba4369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'academic', 'year', 'in', 'Anthropology', '(60', 'EC),', 'Theory', 'and', 'History', 'of', 'Anthropology', '(THA)', 'and/or', 'Historical', 'and', 'Comparative', 'Sociology', '(HCS),', 'a', 'Specialisation', 'Course', 'and', 'Practicing', 'Ethnography']\n",
      "EC),\n"
     ]
    }
   ],
   "source": [
    "# Test: find_credits function\n",
    "\n",
    "upper = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "for info_block in dom.find_all('tr', {'class':''}):\n",
    "    info_block = info_block.text\n",
    "    \n",
    "    # Entry requirements of the course: full text\n",
    "    if 'Entry requirements' in info_block: #Find text box with entry requirements\n",
    "        entry = info_block.replace(\"Entry requirements\", \"\") #Remove title of the text box\n",
    "        entry = ' '.join(entry.split()) #remove multiple whitespaces by splitting and joining\n",
    "        #entry = entry.lower()\n",
    "        \n",
    "        #Split into sentences as these contain seperate criteria\n",
    "        entry = entry.replace(\". \", \".\").split(\".\")\n",
    "        entry = list(filter(None, entry))\n",
    "        \n",
    "        \n",
    "        entry = clean_entry(entry)\n",
    "        #print(entry)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        ### HARD REQUIREMENTS\n",
    "        \n",
    "        cred_l = ['ec', 'ects', 'ect', 'credits', 'ect', 'ecs']\n",
    "        credits = {}\n",
    "      \n",
    "        for sentence in entry:\n",
    "            words = sentence.split()\n",
    "            #print(words)\n",
    "\n",
    "            # COURSE REQUIREMENTS (having completed specific courses)\n",
    "            courses = find_c_names(words) #gives courses of sentence\n",
    "            #print(courses)\n",
    "            \n",
    "#print(entry) #clean entry\n",
    "#print(courses) #courses mentioned in the last sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8d35bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['60 ec from the first-year Psychology curriculum, including Research Participation', 'Scientific and Statistical Reasoning (7202A701XY), Practical training: Psychological Research (7202A704), as well as ≥ 18 ec of the course package of either Brain & Cognition or Psychological Methods', 'For Brain & Cognition students, the 18 ec must include the course B&C Toolbox: Experimental and Neuroscientific Methods (7202BP04)']\n",
      "{'First year': '60', 'Brain & Cognition or Psychological Methods': '18', 'B&C Toolbox: Experimental and Neuroscientific Methods': '18'}\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION: CREDIT REQUIREMENTS - finished\n",
    "print(entry)\n",
    "\n",
    "\n",
    "def find_credits(entry):\n",
    "    # List of words not capitalised in headings\n",
    "    conjunctions = [\"and\", \"or\", \"but\", \"nor\", \"yet\", \"so\", \"for\", \"&\"]\n",
    "    articles = ['a', 'an', 'the']\n",
    "    prepositions = [\"in\", \"to\", \"of\", \"at\", \"by\", \"up\", \"for\", \"off\", \"on\"]\n",
    "    no_cap = conjunctions + articles + prepositions\n",
    "    years = ['year', 'first-year', 'second-year', 'third-year'] \n",
    "    years_cond = ['first', 'second', 'third', '1st', '2nd', '3rd', 'academic', 'Academic'] #needs to be separate because of problem: Clinical Psychology and the first\n",
    "    \n",
    "    cred_l = ['ec', 'ects', 'ect', 'credits', 'ect', 'ecs']\n",
    "    credits = {}\n",
    "    reverse = False\n",
    "    for sentence in entry:       \n",
    "        if 'ec' or 'ects' or 'ect' or 'credits' or 'ect' or 'ecs' in sentence: #check if the entry contains ec information\n",
    "\n",
    "            words = sentence.split()\n",
    "            for i in range(len(words)):\n",
    "                \n",
    "                if words[i] in cred_l: #find position of 'ec'/...\n",
    "                    n_ecs = words[i-1] #Find the number of credits (element before 'ec')\n",
    "                    #print(f'Position of ec {i}')\n",
    "                    \n",
    "                    if n_ecs[0] == '(':\n",
    "                        n_ecs = n_ecs[1:]\n",
    "                        words = words[::-1]\n",
    "                        i = len(words)-i\n",
    "                        reverse = True\n",
    "                    \n",
    "                    #print(f'words:{words} and word: {words[i]}')\n",
    "            \n",
    "                    co = 1\n",
    "                    x = 1\n",
    "                    \n",
    "                    #Find course or year the credits belong to by going trough the words after ec\n",
    "                    while x == 1:\n",
    "                        next_w = words[i+co] #next word\n",
    "\n",
    "\n",
    "                    # Credits required from first/second/third year\n",
    "                    \n",
    "                        first_l = ['first-year', 'first', '1st'] #XXXX might find 'first'/.. that does not belong to year\n",
    "                        second_l = ['second-year', 'second', '2nd']\n",
    "                        third_l = ['third-year', 'third', '3rd']\n",
    "                        \n",
    "                        if next_w in first_l:\n",
    "                            credits['First year'] = n_ecs # XXX credits[f'First year {'the_study_programme'}']\n",
    "                            x = 0\n",
    "                        elif next_w in second_l:\n",
    "                            credits['Second year'] = n_ecs \n",
    "                            x = 0\n",
    "                        elif next_w in third_l:\n",
    "                            credits['Third year'] = n_ecs \n",
    "                            x = 0\n",
    "                            \n",
    "                            \n",
    "                    # Credits required from a course(/specialisation): find course(/specialisation) name\n",
    "                    \n",
    "                        #check if course name starts by checking capitalisation\n",
    "                        elif next_w[0] in upper:\n",
    "\n",
    "                            co2 = 1\n",
    "                            \n",
    "                            #Find course names consisting of multiple words and conjunctions, articles and prepositions\n",
    "                            while words[i+co+co2][0] in upper or words[i+co+co2] in no_cap or words[i+co+co2] in years or words[i+co+co2] in years_cond:\n",
    "                                next_w += \" \" + words[i+co+co2]\n",
    "                                if (i+co+co2+1) == len(words):\n",
    "                                    #print(words[i+co+co2])\n",
    "                                    #print('end')\n",
    "                                    break\n",
    "                                co2 += 1\n",
    "                            \n",
    "                            #Delete conjunctions/articles/prepositions in the end as these are likely not part of \n",
    "                            #the course name anymore: e.g 'Clinical Psychology and the'\n",
    "                            title = next_w.split()\n",
    "                            if reverse == True: # if reversed because course comes before credits e.g., Research Methods (60), bring back to normal order\n",
    "                                title = title[::-1]\n",
    "                            while len(title) != 0 and title[-1] in no_cap or title[-1] in years_cond: \n",
    "                                del title[-1]\n",
    "                            title = ' '.join(title)\n",
    "                            credits[title] = n_ecs #add title and credits to dictionary\n",
    "                            x = 0\n",
    "\n",
    "                        #one word course name\n",
    "                        else:\n",
    "                            co+=1\n",
    "                    words = sentence.split()\n",
    "\n",
    "    return credits   \n",
    "                \n",
    "print(find_credits(entry))\n",
    "\n",
    "\n",
    "#courses[name]['Entry requirements'] = credits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e28d4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run before testing SCRAPER WORKSPACE: make entry requirements categorical\n",
    "\n",
    "#indicators = 'priority', 'ec', 'first year', 'research participation', 'only', 'selected into', \n",
    "#              'completed', 'second year', course ids, study programme names\n",
    "\n",
    "url = f'https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/100604'\n",
    "html = simple_get(url)\n",
    "dom = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#XXX https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/102595 priority\n",
    "#XXX https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/100604\n",
    "#XXX https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/96169\n",
    "#xxx https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/96129\n",
    "#XXX https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/94883\n",
    "#XXX https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/101965 \n",
    "#solved https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/100604 doesnt give credits \n",
    "\n",
    "\n",
    "#XXX https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/94538 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05a783ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first academic year in Anthropology': '60'}\n",
      "['first academic year in Anthropology', 'Theory and History of Anthropology', 'Historical and Comparative Sociology', 'Specialisation Course and Practicing Ethnography']\n"
     ]
    }
   ],
   "source": [
    "# FINAL SCRAPER: ENTRY REQUIREMENTS \n",
    "# of one course\n",
    "\n",
    "upper = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "for info_block in dom.find_all('tr', {'class':''}):\n",
    "    info_block = info_block.text\n",
    "    \n",
    "    # Entry requirements of the course: full text\n",
    "    if 'Entry requirements' in info_block: #Find text box with entry requirements\n",
    "        entry = info_block.replace(\"Entry requirements\", \"\") #Remove title of the text box\n",
    "        entry = ' '.join(entry.split()) #remove multiple whitespaces by splitting and joining\n",
    "        #entry = entry.lower()\n",
    "        \n",
    "        #Split into sentences as these contain seperate criteria\n",
    "        entry = entry.replace(\". \", \".\").split(\".\")\n",
    "        entry = list(filter(None, entry))\n",
    "        \n",
    "        \n",
    "        entry = clean_entry(entry)\n",
    "        #print(entry)\n",
    "\n",
    "        ### HARD REQUIREMENTS\n",
    "        # CREDIT REQUIREMENTS \n",
    "        cred_l = ['ec', 'ects', 'ect', 'credits', 'ect', 'ecs']\n",
    "        credits = {}\n",
    "        credits_dict = find_credits(entry)\n",
    "        #print(f'Credits for courses: {credits_dict}')\n",
    "        print(credits_dict)\n",
    "        \n",
    "\n",
    "        # COURSE REQUIREMENTS (having completed specific courses); XXX could be moved into function\n",
    "        #courses = {}\n",
    "        req_courses = []\n",
    "        for sentence in entry:\n",
    "            words = sentence.split()\n",
    "            new_courses = find_c_names(words)\n",
    "            #print(f'New courses mentioned: {new_courses}')\n",
    "            req_courses += new_courses\n",
    "        #print(f'All courses mentioned: {req_courses}')\n",
    "        #courses['Courses required'] = req_courses\n",
    "        print(req_courses)\n",
    "\n",
    "        \n",
    "                \n",
    "        ### SOFT REQUIREMENTS    \n",
    "            # PRIORISATIONS\n",
    "\n",
    "                \n",
    "                \n",
    "#courses[name]['Entry requirements'] = credits\n",
    "#courses[name]['Entry requirements'] = \n",
    "#courses[name]['Entry requirements'] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2350fc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website doesnt exist: 94501\n",
      "Website exists: 94502\n",
      "Website doesnt exist: 94503\n",
      "Website exists: 94504\n",
      "Website doesnt exist: 94505\n",
      "Website exists: 94506\n",
      "Website doesnt exist: 94507\n",
      "Website doesnt exist: 94508\n",
      "Website doesnt exist: 94509\n",
      "Website doesnt exist: 94510\n",
      "Website doesnt exist: 94511\n",
      "Website exists: 94512\n",
      "Website doesnt exist: 94513\n",
      "Website exists: 94514\n",
      "Website doesnt exist: 94515\n",
      "Website doesnt exist: 94516\n",
      "Website doesnt exist: 94517\n",
      "Website exists: 94518\n",
      "Website doesnt exist: 94519\n",
      "Website exists: 94520\n",
      "Website doesnt exist: 94521\n",
      "Website exists: 94522\n",
      "Website doesnt exist: 94523\n",
      "Website exists: 94524\n",
      "Website doesnt exist: 94525\n",
      "Website exists: 94526\n",
      "Website exists: 94527\n",
      "Website doesnt exist: 94528\n",
      "Website doesnt exist: 94529\n",
      "Website exists: 94530\n",
      "Website doesnt exist: 94531\n",
      "Website exists: 94532\n",
      "Website doesnt exist: 94533\n",
      "Website exists: 94534\n",
      "Website doesnt exist: 94535\n",
      "Website exists: 94536\n",
      "Website doesnt exist: 94537\n",
      "Website exists: 94538\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [93], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m cred_l \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mec\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mects\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mect\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcredits\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mect\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     80\u001b[0m credits \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 81\u001b[0m credits_dict \u001b[38;5;241m=\u001b[39m \u001b[43mfind_credits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#print(f'Credits for courses: {credits_dict}')\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#print(credits_dict)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# COURSE REQUIREMENTS (having completed specific courses); XXX could be moved into function\u001b[39;00m\n\u001b[1;32m     87\u001b[0m req_courses \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn [65], line 41\u001b[0m, in \u001b[0;36mfind_credits\u001b[0;34m(entry)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#Find course or year the credits belong to by going trough the words after ec\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 41\u001b[0m     next_w \u001b[38;5;241m=\u001b[39m \u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mco\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m#next word\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Credits required from first/second/third year\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     first_l \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst-year\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1st\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m#XXXX might find 'first'/.. that does not belong to year\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#delete and continue on Scraper 2\n",
    "#FINAL SCRAPER: name, urls, levels and fields (subset of the data) \n",
    "\n",
    "potential_levels = [\"Bachelor's\", \"Dual Master's\", \"Master's\", \"Exchange Programme Exchange Programme UvA\", \n",
    "                    \"Exchange programme Exchange Programme\", \"Exchange programme Exchange Programme UvA\", \n",
    "                    \"Exchange Programme Exchange Programme\", \"pre-Master's programme\", \"Secondary Subject\", \n",
    "                    \"Bijvak\", \"Honours MSc\", \"Research Master's\", \"Minor\"] #tailored to english website\n",
    "upper = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "\n",
    "courses = {}\n",
    "unique_n = 94500\n",
    "    \n",
    "while unique_n < 94550:\n",
    "    url = f'https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/{unique_n}'\n",
    "    html = simple_get(url)\n",
    "    dom = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    unique_n += 1\n",
    "\n",
    "    if (dom.find('article', class_ = 'twelve columns')) is None: #no course under this url - go to next url\n",
    "        print(f'Website doesnt exist: {unique_n}')\n",
    "        pass\n",
    "\n",
    "    else: #course exists\n",
    "        print(f'Website exists: {unique_n}')\n",
    "        name = dom.find('article', class_ = 'twelve columns').h1.text[1::]\n",
    "        courses[name] = {}\n",
    "        #print(name)\n",
    "        courses[name]['Academic level'] = [] #Add empty list in case there is no data\n",
    "        courses[name]['Field of study'] = [] #Add empty list in case there is no data\n",
    "        courses[name]['Entry requirements'] = [] #Add empty list in case there is no data\n",
    "        courses[name]['url'] = url\n",
    "\n",
    "        #print(url)\n",
    "        \n",
    "        for info_block in dom.find_all('tr', {'class':''}):\n",
    "            info_block = info_block.text\n",
    "\n",
    "\n",
    "            #Education level and field of the course\n",
    "            if 'Is part of' in info_block: #Find text box that contains level and field of the course\n",
    "                is_part_of = info_block.replace(\"Is part of\", \"\") #Remove title of the text box\n",
    "\n",
    "                # Education level of the course e.g., Bachelor's\n",
    "                levels = [] \n",
    "                for l in potential_levels:\n",
    "                    if l in is_part_of:\n",
    "                        if 'Exchange' in l: #unified way of describing and exchange programme\n",
    "                            l = 'Exchange programme'\n",
    "                        levels.append(l)\n",
    "\n",
    "                # Field(s) of the course e.g., Business\n",
    "                for lev in levels: \n",
    "                    is_part_of = is_part_of.replace(lev+\" \", \";\") #replace education level by semicolon to split on the semicolon later\n",
    "                fields = is_part_of[1::].split(\";\") #remove first semicolon to not get an empty element in the study fields list \n",
    "                                                    #and split on semicolon to get this list\n",
    "        \n",
    "                courses[name]['Field of study'] = fields\n",
    "                courses[name]['Academic level'] = levels\n",
    "        \n",
    "\n",
    "            # Entry requirements of the course: full text\n",
    "            if 'Entry requirements' in info_block: #Find text box with entry requirements\n",
    "                entry = info_block.replace(\"Entry requirements\", \"\") #Remove title of the text box\n",
    "                entry = ' '.join(entry.split()) #remove multiple whitespaces by splitting and joining\n",
    "                #entry = entry.lower()\n",
    "\n",
    "                #Split into sentences as these contain seperate criteria\n",
    "                entry = entry.replace(\". \", \".\").split(\".\")\n",
    "                entry = list(filter(None, entry))\n",
    "\n",
    "\n",
    "                entry = clean_entry(entry)\n",
    "                #print(entry)\n",
    "\n",
    "                ### HARD REQUIREMENTS\n",
    "                # CREDIT REQUIREMENTS \n",
    "                cred_l = ['ec', 'ects', 'ect', 'credits', 'ect', 'ecs']\n",
    "                credits = {}\n",
    "                credits_dict = find_credits(entry)\n",
    "                #print(f'Credits for courses: {credits_dict}')\n",
    "                #print(credits_dict)\n",
    "\n",
    "\n",
    "                # COURSE REQUIREMENTS (having completed specific courses); XXX could be moved into function\n",
    "                req_courses = []\n",
    "                for sentence in entry:\n",
    "                    words = sentence.split()\n",
    "                    new_courses = find_c_names(words)\n",
    "                    #print(f'New courses mentioned: {new_courses}')\n",
    "                    req_courses += new_courses\n",
    "                #print(req_courses)\n",
    "                entry_req = {}\n",
    "                entry_req['Required credits'] = credits_dict\n",
    "                entry_req['Required modules'] = req_courses\n",
    "                courses[name]['Entry requirements'] = entry_req\n",
    "\n",
    "print(courses.keys())\n",
    "print('')\n",
    "print(courses)\n",
    "#for name in courses.keys():\n",
    "#    print(courses[name]['Entry requirements'])\n",
    "#    print(courses[name]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "834edd96",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2388921358.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [21], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/102595 #(MA) or (BA)\u001b[0m\n\u001b[0m                                                                                                  ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# EXTRAS\n",
    "\n",
    "url = f'https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/102595'\n",
    "html = simple_get(url)\n",
    "dom = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "for info_block in dom.find_all('tr', {'class':''}):\n",
    "    info_block = info_block.text\n",
    "\n",
    "    # Coordinator information (optional)\n",
    "    if 'Lecturer(s)' in info_block:\n",
    "        #print(info_block) #coord, coord_mail\n",
    "\n",
    "        \n",
    "        \n",
    "#def extract_courses: \n",
    "\n",
    "\n",
    "\n",
    "#Special cases\n",
    "\n",
    "https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/102595 #(MA) or (BA)\n",
    "https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/94517 #Dual Master's / Master's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f0299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
